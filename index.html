<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AI Mood Detector</title>
  <style>
    body {
      background: #0a0a0a;
      color: #00f2ff;
      font-family: "Orbitron", sans-serif;
      text-align: center;
      margin: 0;
      overflow: hidden;
    }
    video, canvas {
      position: absolute;
      left: 50%;
      transform: translateX(-50%);
      top: 60px;
      border: 3px solid #00f2ff;
      border-radius: 16px;
      box-shadow: 0 0 25px #00f2ff;
    }
    h1 {
      margin: 20px 0 0;
      font-size: 28px;
      text-shadow: 0 0 15px #00f2ff;
    }
    #mood {
      margin-top: 540px;
      font-size: 22px;
      color: #ff00ff;
      text-shadow: 0 0 20px #ff00ff, 0 0 40px #ff00ff;
    }
    #history {
      margin-top: 15px;
      font-size: 14px;
      color: #888;
      max-width: 90%;
      margin-left: auto;
      margin-right: auto;
      word-wrap: break-word;
    }
  </style>
</head>
<body>
  <h1>⚡ AI Mood Detector ⚡</h1>
  <video id="video" width="480" height="360" autoplay muted></video>
  <canvas id="overlay" width="480" height="360"></canvas>
  <div id="mood">Detecting mood...</div>
  <div id="history"></div>

  <!-- Face API via CDN -->
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
  <script>
    const video = document.getElementById("video");
    const overlay = document.getElementById("overlay");
    const ctx = overlay.getContext("2d");
    const moodEl = document.getElementById("mood");
    const historyEl = document.getElementById("history");
    let history = [];

    async function start() {
      moodEl.textContent = "Loading models...";
      await faceapi.nets.tinyFaceDetector.loadFromUri("https://cdn.jsdelivr.net/npm/face-api.js/weights");
      await faceapi.nets.faceExpressionNet.loadFromUri("https://cdn.jsdelivr.net/npm/face-api.js/weights");

      const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
      video.srcObject = stream;
      moodEl.textContent = "Models loaded. Detecting mood...";
    }

    video.addEventListener("play", () => {
      const displaySize = { width: video.width, height: video.height };
      faceapi.matchDimensions(overlay, displaySize);

      setInterval(async () => {
        const detections = await faceapi
          .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
          .withFaceExpressions();

        ctx.clearRect(0, 0, overlay.width, overlay.height);
        const resized = faceapi.resizeResults(detections, displaySize);
        faceapi.draw.drawDetections(overlay, resized);
        faceapi.draw.drawFaceExpressions(overlay, resized);

        if (detections.length > 0) {
          const emotions = detections[0].expressions;
          const mood = Object.keys(emotions).reduce((a, b) =>
            emotions[a] > emotions[b] ? a : b
          );
          moodEl.textContent = `You look: ${mood}`;
          history.push(mood);
          if (history.length > 20) history.shift();
          historyEl.textContent = "History: " + history.join(", ");
        } else {
          moodEl.textContent = "No face detected";
        }
      }, 300);
    });

    start();
  </script>
</body>
</html>
